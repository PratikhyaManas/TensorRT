{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:TF1120_GPU]",
      "language": "python",
      "name": "conda-env-TF1120_GPU-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "colab": {
      "name": "Tensorflow_to_TensorRT.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PratikhyaManas/TensorRT/blob/master/Tensorflow_to_TensorRT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcqgaLv6P3ct",
        "colab_type": "text"
      },
      "source": [
        "## What is TensorRT?\n",
        "\n",
        "TensorRT is an optimization tool provided by NVIDIA that applies graph optimization and layer fusion, and finds the fastest implementation of a deep learning model. In other words, TensorRT will optimize our deep learning model so that we expect a faster inference time than the original model (before optimization), such as 5x faster or 2x faster. The bigger model we have, the bigger space for TensorRT to optimize the model. Furthermore, this TensorRT supports all NVIDIA GPU devices, such as 1080Ti, Titan XP for Desktop, and Jetson TX1, TX2 for embedded device."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T7h9KhcSyzH",
        "colab_type": "text"
      },
      "source": [
        "# Importing the Required Libraries and Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WkYWnYaP3cv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "fc3e8bdc-a886-45f1-e217-edefb95b095a"
      },
      "source": [
        "# Import the needed libraries\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.tensorrt as trt\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E-tYH4OP3cv",
        "colab_type": "text"
      },
      "source": [
        "# Convert Tensorflow Model to Frozen Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Ca_q0nRgTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f2cbe0ff-b9ae-4432-aed1-8c33ca505554"
      },
      "source": [
        "# has to be use this setting to make a session for TensorRT optimization\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
        "    # import the meta graph of the tensorflow model\n",
        "    #saver = tf.train.import_meta_graph(\"./model/tensorflow/big/model1.meta\")\n",
        "    saver = tf.train.import_meta_graph(\"/content/drive/My Drive/ML_Datasets/model/tensorflow/small/model_small.meta\")\n",
        "    # then, restore the weights to the meta graph\n",
        "    #saver.restore(sess, \"./model/tensorflow/big/model1\")\n",
        "    saver.restore(sess, \"/content/drive/My Drive/ML_Datasets/model/tensorflow/small/model_small\")\n",
        "    \n",
        "    # specify which tensor output you want to obtain \n",
        "    # (correspond to prediction result)\n",
        "    your_outputs = [\"output_tensor/Softmax\"]\n",
        "    \n",
        "    # convert to frozen model\n",
        "    frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
        "        sess, # session\n",
        "        tf.get_default_graph().as_graph_def(),# graph+weight from the session\n",
        "        output_node_names=your_outputs)\n",
        "    #write the TensorRT model to be used later for inference\n",
        "    with gfile.FastGFile(\"/content/drive/My Drive/ML_Datasets/model/frozen_model.pb\", 'wb') as f:\n",
        "        f.write(frozen_graph.SerializeToString())\n",
        "    print(\"Frozen model is successfully stored!\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/ML_Datasets/model/tensorflow/small/model_small\n",
            "WARNING:tensorflow:From <ipython-input-3-12ba0b412b78>:17: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 10 variables.\n",
            "INFO:tensorflow:Converted 10 variables to const ops.\n",
            "WARNING:tensorflow:From <ipython-input-3-12ba0b412b78>:19: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.gfile.GFile.\n",
            "Frozen model is successfully stored!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnphXIxrP3cz",
        "colab_type": "text"
      },
      "source": [
        "# Optimize the frozen model to TensorRT graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLzjZudPP3cz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "003d22ff-c067-4e43-98e1-fd97e7fb0f5c"
      },
      "source": [
        "# convert (optimize) frozen model to TensorRT model\n",
        "trt_graph = trt.create_inference_graph(\n",
        "    input_graph_def=frozen_graph,# frozen model\n",
        "    outputs=your_outputs,\n",
        "    max_batch_size=2,# specify your max batch size\n",
        "    max_workspace_size_bytes=2*(10**9),# specify the max workspace\n",
        "    precision_mode=\"FP32\") # precision, can be \"FP32\" (32 floating point precision) or \"FP16\"\n",
        "\n",
        "#write the TensorRT model to be used later for inference\n",
        "with gfile.FastGFile(\"/content/drive/My Drive/ML_Datasets/model/TensorRT_model.pb\", 'wb') as f:\n",
        "    f.write(trt_graph.SerializeToString())\n",
        "print(\"TensorRT model is successfully stored!\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n",
            "INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n",
            "INFO:tensorflow:Running against TensorRT version 0.0.0\n",
            "TensorRT model is successfully stored!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48Bn4o_vP3c2",
        "colab_type": "text"
      },
      "source": [
        "# Count how many nodes/operations before and after optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJxrfSiNP3c2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ea572b4f-4b9a-49fd-aa3c-b75fddd8f213"
      },
      "source": [
        "# check how many ops of the original frozen model\n",
        "all_nodes = len([1 for n in frozen_graph.node])\n",
        "print(\"numb. of all_nodes in frozen graph:\", all_nodes)\n",
        "\n",
        "# check how many ops that is converted to TensorRT engine\n",
        "trt_engine_nodes = len([1 for n in trt_graph.node if str(n.op) == 'TRTEngineOp'])\n",
        "print(\"numb. of trt_engine_nodes in TensorRT graph:\", trt_engine_nodes)\n",
        "all_nodes = len([1 for n in trt_graph.node])\n",
        "print(\"numb. of all_nodes in TensorRT graph:\", all_nodes)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numb. of all_nodes in frozen graph: 46\n",
            "numb. of trt_engine_nodes in TensorRT graph: 0\n",
            "numb. of all_nodes in TensorRT graph: 41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHLIHxnYP3c5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}